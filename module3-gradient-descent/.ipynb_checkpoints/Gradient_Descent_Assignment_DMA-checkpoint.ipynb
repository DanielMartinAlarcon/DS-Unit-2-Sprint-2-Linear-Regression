{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qhm0Y_jqXKRv"
   },
   "source": [
    "# Gradient Descent Implementation Challenge!!\n",
    "\n",
    "## Use gradient descent to find the optimal parameters of a **multiple** regression model. (We only showed an implementation for a bivariate model during lecture.)\n",
    "\n",
    "A note: Implementing gradient descent in any context is not trivial, particularly the step where we calculate the gradient will change based on the number of parameters that we're trying to optimize for. You will need to research what the gradient of a multiple regression model looks like. This challenge is pretty open-ended but I hope it will be thrilling. Please work together, help each other, share resources and generally expand your understanding of gradient descent as you try and achieve this implementation. \n",
    "\n",
    "## Suggestions:\n",
    "\n",
    "Start off with a model that has just two $X$ variables You can use any datasets that have at least two x variables. Potential candidates might be the blood pressure dataset that we used during lecture on Monday: [HERE](https://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/mlr/excel/mlr02.xls) or any of the housing datasets. You would just need to select from them the two varaibles $x$ variables and one y variable that you want to work with that you most want to work with. \n",
    "\n",
    "Use Sklearn to find the optimal parameters of your model first. (like we did during the lecture.) So that you can compare the parameter estimates of your gradient-descent linear regression to the estimates of OLS linear regression. If implemented correctly they should be nearly identical.\n",
    "\n",
    "Becoming a Data Scientist is all about striking out into the unknown, getting stuck and then researching and fighting and learning until you get yourself unstuck. Work together! And fight to take your own learning-rate fueled step towards your own optimal understanding of gradient descent! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_tWzF6IqXIIq"
   },
   "outputs": [],
   "source": [
    "# Incantations\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# To display all cell outputs instead of just last\n",
    "# No more needing to always print()\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLS Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** No CODEPAGE record, no encoding_override: will use 'ascii'\n",
      "(11, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>age</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>132</td>\n",
       "      <td>52</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>143</td>\n",
       "      <td>59</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>153</td>\n",
       "      <td>67</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>162</td>\n",
       "      <td>73</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>154</td>\n",
       "      <td>64</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y  age  weight\n",
       "0  132   52     173\n",
       "1  143   59     184\n",
       "2  153   67     194\n",
       "3  162   73     211\n",
       "4  154   64     196"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing Monday's blood pressure dataset.\n",
    "df = pd.read_excel('https://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/mlr/excel/mlr02.xls')\n",
    "df = df.rename(index=str, columns={\"X1\": \"y\", \"X2\": \"age\", \"X3\": \"weight\"})\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Intercept Value, beta_0: \n",
      " [30.99410295]\n",
      "\n",
      "Slope Coefficients, beta_i: \n",
      " [0.86141469 0.3348592 ]\n",
      "\n",
      "Blood pressure, aka \"targets\" and y: \n",
      "[[132]\n",
      " [143]\n",
      " [153]\n",
      " [162]\n",
      " [154]\n",
      " [168]\n",
      " [137]\n",
      " [149]\n",
      " [159]\n",
      " [128]\n",
      " [166]]\n",
      "\n",
      "Independent variables, X: \n",
      "[[ 52 173]\n",
      " [ 59 184]\n",
      " [ 67 194]\n",
      " [ 73 211]\n",
      " [ 64 196]\n",
      " [ 74 220]\n",
      " [ 54 188]\n",
      " [ 61 188]\n",
      " [ 65 207]\n",
      " [ 46 167]\n",
      " [ 72 217]]\n"
     ]
    }
   ],
   "source": [
    "y = df.loc[:, ['y']].values\n",
    "X = df.loc[:, ['age','weight']].values\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "beta_0 = model.intercept_\n",
    "beta_i = model.coef_[0]\n",
    "\n",
    "print(\"\\nIntercept Value, beta_0: \\n\", beta_0)\n",
    "print(\"\\nSlope Coefficients, beta_i: \\n\", beta_i)\n",
    "print(f'\\nBlood pressure, aka \"targets\" and y: \\n{y}')\n",
    "print(f'\\nIndependent variables, X: \\n{X}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent formula is given by:\n",
    "![Gradient Descent formula](http://www.ryanleeallred.com/wp-content/uploads/2019/01/gradient-descent-formula.png)\n",
    "Where each new theta_j results from subtracting a term composed of alpha and the slope of J evaluated at the current theta_j. Theta is the vector of coefficients that go into the linear regression. Theta = [beta_0, beta_i[0], ... beta_i[total dimensions - 1]].\n",
    "\n",
    "The slope is something we'll determine soon, but is based on the error evaluated at theta_j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implemented part-by-part\n",
    "I'll do this by parts first, to make sure that I understand what's going on before putting everything into a singel function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_null: \n",
      "[[60]\n",
      " [ 5]\n",
      " [ 5]]\n",
      "theta_null shape: \n",
      "(3, 1)\n"
     ]
    }
   ],
   "source": [
    "# theta_null is the starting point for the equation, in this case chosen \n",
    "# because it's close to the global maximum given by OLS. \n",
    "theta_null = np.array([60, 5, 5]).reshape(3,-1)\n",
    "print(f'theta_null: \\n{theta_null}')\n",
    "print(f'theta_null shape: \\n{theta_null.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_norm: \n",
      "[[-1.20301838 -1.33261043]\n",
      " [-0.39751912 -0.66630522]\n",
      " [ 0.52305147 -0.0605732 ]\n",
      " [ 1.21347941  0.96917122]\n",
      " [ 0.1778375   0.0605732 ]\n",
      " [ 1.32855074  1.51433004]\n",
      " [-0.97287574 -0.42401241]\n",
      " [-0.16737647 -0.42401241]\n",
      " [ 0.29290882  0.72687842]\n",
      " [-1.89344632 -1.69604964]\n",
      " [ 1.09840809  1.33261043]]\n"
     ]
    }
   ],
   "source": [
    "# I first normalize X\n",
    "\n",
    "# StandardScaler subtracts mean, normalizes to stdev=1\n",
    "scaler = StandardScaler() \n",
    "# I apply the normalization to X. I changed the type to prevent a \n",
    "# dtype change warning from StandardScaler\n",
    "X_norm = scaler.fit_transform(X.astype('float64'))\n",
    "print(f'X_norm: \\n{X_norm}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_linalg: \n",
      "[[ 1.         -1.20301838 -1.33261043]\n",
      " [ 1.         -0.39751912 -0.66630522]\n",
      " [ 1.          0.52305147 -0.0605732 ]\n",
      " [ 1.          1.21347941  0.96917122]\n",
      " [ 1.          0.1778375   0.0605732 ]\n",
      " [ 1.          1.32855074  1.51433004]\n",
      " [ 1.         -0.97287574 -0.42401241]\n",
      " [ 1.         -0.16737647 -0.42401241]\n",
      " [ 1.          0.29290882  0.72687842]\n",
      " [ 1.         -1.89344632 -1.69604964]\n",
      " [ 1.          1.09840809  1.33261043]]\n"
     ]
    }
   ],
   "source": [
    "# I then add a column of 1s at the beginning, destined to later be \n",
    "# multiplied by beta_0. I use the class parameter 'np.c_', which\n",
    "# concatenates two arrays along the second axis (columns) and creates such\n",
    "# an axis if it doesn't exist (as happens here with the np.ones \n",
    "# vector that has the same length as the rows in X_norm)\n",
    "X_linalg = np.c_[np.ones(X_norm.shape[0]), X_norm]\n",
    "print(f'X_linalg: \\n{X_linalg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat: \n",
      "[[47.32185592]\n",
      " [54.68087833]\n",
      " [62.31239135]\n",
      " [70.91325318]\n",
      " [61.19205351]\n",
      " [74.21440387]\n",
      " [53.01555927]\n",
      " [57.04305559]\n",
      " [65.09893621]\n",
      " [42.05252017]\n",
      " [72.15509261]]\n"
     ]
    }
   ],
   "source": [
    "# I then make an array of predictions, y_hat, which for any \n",
    "# datapoint (row) in X_linalg is the result of multiplying all the terms\n",
    "# by the regression equation coefficients in theta and adding up\n",
    "# these products.  That is, the regression equation is evaluated at each\n",
    "# row in X, which may contain multiple dimensions. The simple way to \n",
    "# calculate that is with the dot product. I'll include these calculations\n",
    "# in a general function later, but for now I'll use the initialization\n",
    "# values.\n",
    "theta = theta_null\n",
    "y_hat = np.dot(X_linalg, theta)\n",
    "print(f'y_hat: \\n{y_hat}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use 0.5* Mean Square Error as our cost function, as described [here](https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#id3), then the formula for the gradient at any given point is given by:\n",
    "\n",
    "$gradient = (-X*(errors))/N$\n",
    "\n",
    "where\n",
    "\n",
    "$errors = targets-predictions = y - \\hat{y}$ \n",
    "\n",
    "$N$ is the number of datapoints, and we're taking the dot product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "errors: \n",
      "[[84.67814408]\n",
      " [88.31912167]\n",
      " [90.68760865]\n",
      " [91.08674682]\n",
      " [92.80794649]\n",
      " [93.78559613]\n",
      " [83.98444073]\n",
      " [91.95694441]\n",
      " [93.90106379]\n",
      " [85.94747983]\n",
      " [93.84490739]]\n"
     ]
    }
   ],
   "source": [
    "# The error comes from subtracting the actual y values from our \n",
    "# predictions, y_hat. Note that I had to reshape them so that the\n",
    "# subtraction happened elementwise.\n",
    "errors = y - y_hat\n",
    "print(f'errors: \\n{errors}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[90.09090909],\n",
       "       [ 2.98559946],\n",
       "       [ 2.87998194]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient = np.dot(X_linalg.T, errors) / len(y)\n",
    "# gradient = gradient.reshape(1,-1)\n",
    "# print(f'gradient: \\n{gradient}')\n",
    "gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[50.99090909],\n",
       "       [ 4.70144005],\n",
       "       [ 4.71200181]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And once we have the gradient, we update theta by subtracting the\n",
    "# gradient times the learning rate alpha.\n",
    "alpha = 0.1\n",
    "theta = theta_null - alpha*gradient\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta_null, iterations, alpha):\n",
    "    \"\"\"\n",
    "    Implements gradient descent\n",
    "    \n",
    "    X = array of variables\n",
    "    y = array of results\n",
    "    theta_null = initial guesses for the vector of coefficients, theta\n",
    "    iterations = how many steps to take in the descent\n",
    "    alpha = learning rate for each step\n",
    "    \"\"\"\n",
    "    theta = theta_null\n",
    "    scaler = StandardScaler()\n",
    "    X_norm = scaler.fit_transform(X.astype('float64'))\n",
    "    X_linalg = np.c_[np.ones(X_norm.shape[0]), X_norm]\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        y_hat = np.dot(X_linalg, theta)\n",
    "        errors = y - y_hat\n",
    "        gradient = np.dot(X_linalg.T, errors) / len(y)\n",
    "        theta = theta - alpha*gradient\n",
    "        print(theta)\n",
    "        print()\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[60],\n",
       "       [ 5],\n",
       "       [ 5]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.loc[:, ['age','weight']].values\n",
    "y = df.loc[:, ['y']].values\n",
    "np.array([60, 5, 5]).reshape(3,-1)\n",
    "iterations = 200\n",
    "alpha = 0.01\n",
    "theta_null = np.array([60, 5, 5]).reshape(3,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[59.09909091]\n",
      " [ 4.97014401]\n",
      " [ 4.97120018]]\n",
      "\n",
      "[[58.18917273]\n",
      " [ 4.93971699]\n",
      " [ 4.94182991]]\n",
      "\n",
      "[[57.27015536]\n",
      " [ 4.90870785]\n",
      " [ 4.91187808]]\n",
      "\n",
      "[[56.34194783]\n",
      " [ 4.87710525]\n",
      " [ 4.88133337]]\n",
      "\n",
      "[[55.40445821]\n",
      " [ 4.84489766]\n",
      " [ 4.85018424]]\n",
      "\n",
      "[[54.4575937 ]\n",
      " [ 4.81207331]\n",
      " [ 4.81841891]]\n",
      "\n",
      "[[53.50126055]\n",
      " [ 4.7786202 ]\n",
      " [ 4.7860254 ]]\n",
      "\n",
      "[[52.53536407]\n",
      " [ 4.74452609]\n",
      " [ 4.75299147]]\n",
      "\n",
      "[[51.55980862]\n",
      " [ 4.70977853]\n",
      " [ 4.71930465]]\n",
      "\n",
      "[[50.57449761]\n",
      " [ 4.6743648 ]\n",
      " [ 4.68495223]]\n",
      "\n",
      "[[49.5793335 ]\n",
      " [ 4.63827194]\n",
      " [ 4.64992126]]\n",
      "\n",
      "[[48.57421774]\n",
      " [ 4.60148674]\n",
      " [ 4.61419852]]\n",
      "\n",
      "[[47.55905083]\n",
      " [ 4.56399573]\n",
      " [ 4.57777054]]\n",
      "\n",
      "[[46.53373224]\n",
      " [ 4.52578519]\n",
      " [ 4.5406236 ]]\n",
      "\n",
      "[[45.49816048]\n",
      " [ 4.48684111]\n",
      " [ 4.50274371]]\n",
      "\n",
      "[[44.45223299]\n",
      " [ 4.44714922]\n",
      " [ 4.46411658]]\n",
      "\n",
      "[[43.39584623]\n",
      " [ 4.40669499]\n",
      " [ 4.42472767]]\n",
      "\n",
      "[[42.3288956 ]\n",
      " [ 4.36546357]\n",
      " [ 4.38456216]]\n",
      "\n",
      "[[41.25127546]\n",
      " [ 4.32343985]\n",
      " [ 4.34360492]]\n",
      "\n",
      "[[40.16287913]\n",
      " [ 4.28060842]\n",
      " [ 4.30184054]]\n",
      "\n",
      "[[39.06359883]\n",
      " [ 4.23695356]\n",
      " [ 4.25925331]]\n",
      "\n",
      "[[37.95332573]\n",
      " [ 4.19245926]\n",
      " [ 4.21582721]]\n",
      "\n",
      "[[36.83194989]\n",
      " [ 4.14710918]\n",
      " [ 4.17154591]]\n",
      "\n",
      "[[35.6993603 ]\n",
      " [ 4.10088668]\n",
      " [ 4.12639277]]\n",
      "\n",
      "[[34.55544481]\n",
      " [ 4.05377477]\n",
      " [ 4.0803508 ]]\n",
      "\n",
      "[[33.40009017]\n",
      " [ 4.00575617]\n",
      " [ 4.03340271]]\n",
      "\n",
      "[[32.23318198]\n",
      " [ 3.95681323]\n",
      " [ 3.98553086]]\n",
      "\n",
      "[[31.05460471]\n",
      " [ 3.90692797]\n",
      " [ 3.93671727]]\n",
      "\n",
      "[[29.86424167]\n",
      " [ 3.85608205]\n",
      " [ 3.8869436 ]]\n",
      "\n",
      "[[28.66197499]\n",
      " [ 3.80425679]\n",
      " [ 3.83619116]]\n",
      "\n",
      "[[27.44768565]\n",
      " [ 3.75143314]\n",
      " [ 3.78444091]]\n",
      "\n",
      "[[26.22125342]\n",
      " [ 3.69759166]\n",
      " [ 3.73167341]]\n",
      "\n",
      "[[24.98255686]\n",
      " [ 3.64271256]\n",
      " [ 3.67786887]]\n",
      "\n",
      "[[23.73147334]\n",
      " [ 3.58677565]\n",
      " [ 3.62300711]]\n",
      "\n",
      "[[22.46787898]\n",
      " [ 3.52976035]\n",
      " [ 3.56706753]]\n",
      "\n",
      "[[21.19164868]\n",
      " [ 3.47164568]\n",
      " [ 3.51002916]]\n",
      "\n",
      "[[19.90265607]\n",
      " [ 3.41241025]\n",
      " [ 3.45187061]]\n",
      "\n",
      "[[18.60077354]\n",
      " [ 3.35203226]\n",
      " [ 3.39257009]]\n",
      "\n",
      "[[17.28587219]\n",
      " [ 3.29048947]\n",
      " [ 3.33210534]]\n",
      "\n",
      "[[15.95782182]\n",
      " [ 3.22775923]\n",
      " [ 3.27045373]]\n",
      "\n",
      "[[14.61649095]\n",
      " [ 3.16381843]\n",
      " [ 3.20759214]]\n",
      "\n",
      "[[13.26174677]\n",
      " [ 3.09864352]\n",
      " [ 3.14349702]]\n",
      "\n",
      "[[11.89345514]\n",
      " [ 3.03221048]\n",
      " [ 3.07814436]]\n",
      "\n",
      "[[10.5114806 ]\n",
      " [ 2.96449485]\n",
      " [ 3.01150968]]\n",
      "\n",
      "[[9.11568632]\n",
      " [2.89547166]\n",
      " [2.94356803]]\n",
      "\n",
      "[[7.70593409]\n",
      " [2.82511548]\n",
      " [2.87429397]]\n",
      "\n",
      "[[6.28208434]\n",
      " [2.75340037]\n",
      " [2.80366156]]\n",
      "\n",
      "[[4.84399609]\n",
      " [2.68029989]\n",
      " [2.73164437]]\n",
      "\n",
      "[[3.39152696]\n",
      " [2.60578708]\n",
      " [2.65821544]]\n",
      "\n",
      "[[1.92453314]\n",
      " [2.52983447]\n",
      " [2.58334729]]\n",
      "\n",
      "[[0.44286938]\n",
      " [2.45241404]\n",
      " [2.5070119 ]]\n",
      "\n",
      "[[-1.05361102]\n",
      " [ 2.37349723]\n",
      " [ 2.42918073]]\n",
      "\n",
      "[[-2.56505622]\n",
      " [ 2.29305494]\n",
      " [ 2.34982465]]\n",
      "\n",
      "[[-4.09161587]\n",
      " [ 2.21105747]\n",
      " [ 2.26891398]]\n",
      "\n",
      "[[-5.63344112]\n",
      " [ 2.12747457]\n",
      " [ 2.18641847]]\n",
      "\n",
      "[[-7.19068462]\n",
      " [ 2.04227539]\n",
      " [ 2.10230727]]\n",
      "\n",
      "[[-8.76350056]\n",
      " [ 1.95542849]\n",
      " [ 2.01654893]]\n",
      "\n",
      "[[-10.35204465]\n",
      " [  1.8669018 ]\n",
      " [  1.92911138]]\n",
      "\n",
      "[[-11.95647419]\n",
      " [  1.77666263]\n",
      " [  1.83996196]]\n",
      "\n",
      "[[-13.57694803]\n",
      " [  1.68467768]\n",
      " [  1.74906733]]\n",
      "\n",
      "[[-15.2136266 ]\n",
      " [  1.59091297]\n",
      " [  1.65639353]]\n",
      "\n",
      "[[-16.86667195]\n",
      " [  1.49533386]\n",
      " [  1.56190592]]\n",
      "\n",
      "[[-18.53624776]\n",
      " [  1.39790507]\n",
      " [  1.46556922]]\n",
      "\n",
      "[[-20.22251933]\n",
      " [  1.29859059]\n",
      " [  1.36734742]]\n",
      "\n",
      "[[-21.92565362]\n",
      " [  1.19735374]\n",
      " [  1.26720384]]\n",
      "\n",
      "[[-23.64581924]\n",
      " [  1.09415711]\n",
      " [  1.16510107]]\n",
      "\n",
      "[[-25.38318653]\n",
      " [  0.98896257]\n",
      " [  1.06100097]]\n",
      "\n",
      "[[-27.13792748]\n",
      " [  0.88173124]\n",
      " [  0.95486469]]\n",
      "\n",
      "[[-28.91021585]\n",
      " [  0.7724235 ]\n",
      " [  0.84665257]]\n",
      "\n",
      "[[-30.7002271 ]\n",
      " [  0.66099893]\n",
      " [  0.73632423]]\n",
      "\n",
      "[[-32.50813846]\n",
      " [  0.54741636]\n",
      " [  0.62383847]]\n",
      "\n",
      "[[-34.33412894]\n",
      " [  0.43163379]\n",
      " [  0.5091533 ]]\n",
      "\n",
      "[[-36.17837932]\n",
      " [  0.31360841]\n",
      " [  0.39222592]]\n",
      "\n",
      "[[-38.0410722 ]\n",
      " [  0.19329659]\n",
      " [  0.27301268]]\n",
      "\n",
      "[[-39.92239201]\n",
      " [  0.07065383]\n",
      " [  0.1514691 ]]\n",
      "\n",
      "[[-4.18225250e+01]\n",
      " [-5.43652245e-02]\n",
      " [ 2.75498249e-02]]\n",
      "\n",
      "[[-43.74165936]\n",
      " [ -0.18180681]\n",
      " [ -0.09879139]]\n",
      "\n",
      "[[-45.67998505]\n",
      " [ -0.31171806]\n",
      " [ -0.22760168]]\n",
      "\n",
      "[[-47.63769399]\n",
      " [ -0.44414704]\n",
      " [ -0.35892911]]\n",
      "\n",
      "[[-49.61498002]\n",
      " [ -0.57914273]\n",
      " [ -0.49282265]]\n",
      "\n",
      "[[-51.61203891]\n",
      " [ -0.71675508]\n",
      " [ -0.62933226]]\n",
      "\n",
      "[[-53.62906839]\n",
      " [ -0.85703501]\n",
      " [ -0.76850884]]\n",
      "\n",
      "[[-55.66626817]\n",
      " [ -1.00003441]\n",
      " [ -0.91040432]]\n",
      "\n",
      "[[-57.72383994]\n",
      " [ -1.14580622]\n",
      " [ -1.05507159]]\n",
      "\n",
      "[[-59.80198743]\n",
      " [ -1.29440437]\n",
      " [ -1.20256462]]\n",
      "\n",
      "[[-61.9009164 ]\n",
      " [ -1.44588386]\n",
      " [ -1.35293839]]\n",
      "\n",
      "[[-64.02083465]\n",
      " [ -1.60030076]\n",
      " [ -1.50624897]]\n",
      "\n",
      "[[-66.16195209]\n",
      " [ -1.75771222]\n",
      " [ -1.66255352]]\n",
      "\n",
      "[[-68.3244807 ]\n",
      " [ -1.91817653]\n",
      " [ -1.82191031]]\n",
      "\n",
      "[[-70.5086346 ]\n",
      " [ -2.08175307]\n",
      " [ -1.98437874]]\n",
      "\n",
      "[[-72.71463003]\n",
      " [ -2.24850241]\n",
      " [ -2.15001937]]\n",
      "\n",
      "[[-74.94268543]\n",
      " [ -2.41848629]\n",
      " [ -2.31889395]]\n",
      "\n",
      "[[-77.19302137]\n",
      " [ -2.59176765]\n",
      " [ -2.49106541]]\n",
      "\n",
      "[[-79.46586068]\n",
      " [ -2.76841066]\n",
      " [ -2.66659791]]\n",
      "\n",
      "[[-81.76142837]\n",
      " [ -2.94848072]\n",
      " [ -2.84555687]]\n",
      "\n",
      "[[-84.07995175]\n",
      " [ -3.13204453]\n",
      " [ -3.02800898]]\n",
      "\n",
      "[[-86.42166036]\n",
      " [ -3.31917006]\n",
      " [ -3.21402221]]\n",
      "\n",
      "[[-88.78678605]\n",
      " [ -3.50992663]\n",
      " [ -3.40366588]]\n",
      "\n",
      "[[-91.175563  ]\n",
      " [ -3.7043849 ]\n",
      " [ -3.59701064]]\n",
      "\n",
      "[[-93.58822772]\n",
      " [ -3.90261689]\n",
      " [ -3.79412853]]\n",
      "\n",
      "[[-96.02501909]\n",
      " [ -4.10469603]\n",
      " [ -3.99509297]]\n",
      "\n",
      "[[-98.48617837]\n",
      " [ -4.31069719]\n",
      " [ -4.19997883]]\n",
      "\n",
      "[[-100.97194925]\n",
      " [  -4.52069669]\n",
      " [  -4.40886242]]\n",
      "\n",
      "[[-103.48257783]\n",
      " [  -4.73477234]\n",
      " [  -4.62182156]]\n",
      "\n",
      "[[-106.0183127 ]\n",
      " [  -4.95300344]\n",
      " [  -4.83893555]]\n",
      "\n",
      "[[-108.57940492]\n",
      " [  -5.17547086]\n",
      " [  -5.06028526]]\n",
      "\n",
      "[[-111.16610806]\n",
      " [  -5.40225704]\n",
      " [  -5.28595312]]\n",
      "\n",
      "[[-113.77867823]\n",
      " [  -5.63344601]\n",
      " [  -5.51602318]]\n",
      "\n",
      "[[-116.4173741 ]\n",
      " [  -5.86912346]\n",
      " [  -5.7505811 ]]\n",
      "\n",
      "[[-119.08245694]\n",
      " [  -6.10937672]\n",
      " [  -5.98971423]]\n",
      "\n",
      "[[-121.7741906 ]\n",
      " [  -6.35429483]\n",
      " [  -6.23351161]]\n",
      "\n",
      "[[-124.49284159]\n",
      " [  -6.60396857]\n",
      " [  -6.48206402]]\n",
      "\n",
      "[[-127.2386791 ]\n",
      " [  -6.85849049]\n",
      " [  -6.73546399]]\n",
      "\n",
      "[[-130.01197498]\n",
      " [  -7.11795491]\n",
      " [  -6.99380587]]\n",
      "\n",
      "[[-132.81300382]\n",
      " [  -7.38245803]\n",
      " [  -7.25718584]]\n",
      "\n",
      "[[-135.64204295]\n",
      " [  -7.65209789]\n",
      " [  -7.52570194]]\n",
      "\n",
      "[[-138.49937247]\n",
      " [  -7.92697445]\n",
      " [  -7.79945414]]\n",
      "\n",
      "[[-141.38527529]\n",
      " [  -8.20718962]\n",
      " [  -8.07854433]]\n",
      "\n",
      "[[-144.30003713]\n",
      " [  -8.49284727]\n",
      " [  -8.3630764 ]]\n",
      "\n",
      "[[-147.24394659]\n",
      " [  -8.78405331]\n",
      " [  -8.65315626]]\n",
      "\n",
      "[[-150.21729515]\n",
      " [  -9.08091572]\n",
      " [  -8.94889188]]\n",
      "\n",
      "[[-153.22037719]\n",
      " [  -9.38354457]\n",
      " [  -9.25039333]]\n",
      "\n",
      "[[-156.25349006]\n",
      " [  -9.69205206]\n",
      " [  -9.55777281]]\n",
      "\n",
      "[[-159.31693405]\n",
      " [ -10.0065526 ]\n",
      " [  -9.87114473]]\n",
      "\n",
      "[[-162.41101248]\n",
      " [ -10.3271628 ]\n",
      " [ -10.19062571]]\n",
      "\n",
      "[[-165.53603169]\n",
      " [ -10.65400156]\n",
      " [ -10.51633463]]\n",
      "\n",
      "[[-168.6923011 ]\n",
      " [ -10.98719008]\n",
      " [ -10.84839271]]\n",
      "\n",
      "[[-171.8801332 ]\n",
      " [ -11.32685193]\n",
      " [ -11.1869235 ]]\n",
      "\n",
      "[[-175.09984363]\n",
      " [ -11.67311307]\n",
      " [ -11.53205298]]\n",
      "\n",
      "[[-178.35175115]\n",
      " [ -12.02610192]\n",
      " [ -11.88390955]]\n",
      "\n",
      "[[-181.63617776]\n",
      " [ -12.38594941]\n",
      " [ -12.24262415]]\n",
      "\n",
      "[[-184.95344862]\n",
      " [ -12.75278899]\n",
      " [ -12.60833024]]\n",
      "\n",
      "[[-188.3038922 ]\n",
      " [ -13.12675674]\n",
      " [ -12.98116388]]\n",
      "\n",
      "[[-191.68784021]\n",
      " [ -13.50799137]\n",
      " [ -13.36126379]]\n",
      "\n",
      "[[-195.10562771]\n",
      " [ -13.89663428]\n",
      " [ -13.74877137]]\n",
      "\n",
      "[[-198.55759308]\n",
      " [ -14.29282964]\n",
      " [ -14.14383079]]\n",
      "\n",
      "[[-202.0440781 ]\n",
      " [ -14.69672442]\n",
      " [ -14.54658901]]\n",
      "\n",
      "[[-205.56542797]\n",
      " [ -15.10846845]\n",
      " [ -14.95719587]]\n",
      "\n",
      "[[-209.12199134]\n",
      " [ -15.52821447]\n",
      " [ -15.3758041 ]]\n",
      "\n",
      "[[-212.71412034]\n",
      " [ -15.95611821]\n",
      " [ -15.80256944]]\n",
      "\n",
      "[[-216.34217064]\n",
      " [ -16.39233839]\n",
      " [ -16.23765061]]\n",
      "\n",
      "[[-220.00650144]\n",
      " [ -16.83703687]\n",
      " [ -16.68120947]]\n",
      "\n",
      "[[-223.70747554]\n",
      " [ -17.29037864]\n",
      " [ -17.13341099]]\n",
      "\n",
      "[[-227.44545939]\n",
      " [ -17.75253187]\n",
      " [ -17.59442337]]\n",
      "\n",
      "[[-231.22082307]\n",
      " [ -18.22366806]\n",
      " [ -18.06441808]]\n",
      "\n",
      "[[-235.03394039]\n",
      " [ -18.703962  ]\n",
      " [ -18.54356993]]\n",
      "\n",
      "[[-238.88518889]\n",
      " [ -19.1935919 ]\n",
      " [ -19.03205713]]\n",
      "\n",
      "[[-242.77494987]\n",
      " [ -19.69273944]\n",
      " [ -19.53006135]]\n",
      "\n",
      "[[-246.70360846]\n",
      " [ -20.20158983]\n",
      " [ -20.0377678 ]]\n",
      "\n",
      "[[-250.67155363]\n",
      " [ -20.72033189]\n",
      " [ -20.55536531]]\n",
      "\n",
      "[[-254.67917826]\n",
      " [ -21.24915811]\n",
      " [ -21.08304635]]\n",
      "\n",
      "[[-258.72687913]\n",
      " [ -21.78826473]\n",
      " [ -21.62100718]]\n",
      "\n",
      "[[-262.81505702]\n",
      " [ -22.3378518 ]\n",
      " [ -22.16944784]]\n",
      "\n",
      "[[-266.94411668]\n",
      " [ -22.89812327]\n",
      " [ -22.72857228]]\n",
      "\n",
      "[[-271.11446694]\n",
      " [ -23.46928706]\n",
      " [ -23.29858843]]\n",
      "\n",
      "[[-275.3265207 ]\n",
      " [ -24.05155513]\n",
      " [ -23.87970823]]\n",
      "\n",
      "[[-279.58069499]\n",
      " [ -24.64514358]\n",
      " [ -24.4721478 ]]\n",
      "\n",
      "[[-283.87741103]\n",
      " [ -25.25027269]\n",
      " [ -25.07612741]]\n",
      "\n",
      "[[-288.21709424]\n",
      " [ -25.86716705]\n",
      " [ -25.69187165]]\n",
      "\n",
      "[[-292.60017427]\n",
      " [ -26.49605562]\n",
      " [ -26.31960947]]\n",
      "\n",
      "[[-297.0270851 ]\n",
      " [ -27.13717179]\n",
      " [ -26.95957427]]\n",
      "\n",
      "[[-301.49826504]\n",
      " [ -27.79075352]\n",
      " [ -27.61200402]]\n",
      "\n",
      "[[-306.01415679]\n",
      " [ -28.45704339]\n",
      " [ -28.27714128]]\n",
      "\n",
      "[[-310.57520744]\n",
      " [ -29.1362887 ]\n",
      " [ -28.95523336]]\n",
      "\n",
      "[[-315.18186861]\n",
      " [ -29.82874156]\n",
      " [ -29.64653237]]\n",
      "\n",
      "[[-319.83459639]\n",
      " [ -30.53465899]\n",
      " [ -30.35129533]]\n",
      "\n",
      "[[-324.53385144]\n",
      " [ -31.25430302]\n",
      " [ -31.06978426]]\n",
      "\n",
      "[[-329.28009905]\n",
      " [ -31.98794077]\n",
      " [ -31.80226628]]\n",
      "\n",
      "[[-334.07380913]\n",
      " [ -32.73584455]\n",
      " [ -32.54901372]]\n",
      "\n",
      "[[-338.91545631]\n",
      " [ -33.49829198]\n",
      " [ -33.31030418]]\n",
      "\n",
      "[[-343.80551996]\n",
      " [ -34.27556609]\n",
      " [ -34.0864207 ]]\n",
      "\n",
      "[[-348.74448425]\n",
      " [ -35.06795539]\n",
      " [ -34.87765179]]\n",
      "\n",
      "[[-353.73283819]\n",
      " [ -35.87575405]\n",
      " [ -35.6842916 ]]\n",
      "\n",
      "[[-358.77107566]\n",
      " [ -36.69926192]\n",
      " [ -36.50664   ]]\n",
      "\n",
      "[[-363.85969551]\n",
      " [ -37.5387847 ]\n",
      " [ -37.3450027 ]]\n",
      "\n",
      "[[-368.99920155]\n",
      " [ -38.39463406]\n",
      " [ -38.19969134]]\n",
      "\n",
      "[[-374.19010266]\n",
      " [ -39.26712771]\n",
      " [ -39.07102364]]\n",
      "\n",
      "[[-379.43291278]\n",
      " [ -40.15658954]\n",
      " [ -39.9593235 ]]\n",
      "\n",
      "[[-384.728151  ]\n",
      " [ -41.06334976]\n",
      " [ -40.86492113]]\n",
      "\n",
      "[[-390.0763416 ]\n",
      " [ -41.98774501]\n",
      " [ -41.78815315]]\n",
      "\n",
      "[[-395.4780141 ]\n",
      " [ -42.93011846]\n",
      " [ -42.72936275]]\n",
      "\n",
      "[[-400.93370334]\n",
      " [ -43.89081997]\n",
      " [ -43.68889978]]\n",
      "\n",
      "[[-406.44394946]\n",
      " [ -44.87020621]\n",
      " [ -44.66712091]]\n",
      "\n",
      "[[-412.00929805]\n",
      " [ -45.86864079]\n",
      " [ -45.66438975]]\n",
      "\n",
      "[[-417.63030012]\n",
      " [ -46.88649439]\n",
      " [ -46.68107699]]\n",
      "\n",
      "[[-423.30751221]\n",
      " [ -47.92414491]\n",
      " [ -47.71756051]]\n",
      "\n",
      "[[-429.04149642]\n",
      " [ -48.9819776 ]\n",
      " [ -48.77422558]]\n",
      "\n",
      "[[-434.83282048]\n",
      " [ -50.06038522]\n",
      " [ -49.85146494]]\n",
      "\n",
      "[[-440.68205777]\n",
      " [ -51.15976815]\n",
      " [ -50.94967899]]\n",
      "\n",
      "[[-446.58978744]\n",
      " [ -52.28053458]\n",
      " [ -52.0692759 ]]\n",
      "\n",
      "[[-452.55659441]\n",
      " [ -53.42310064]\n",
      " [ -53.21067182]]\n",
      "\n",
      "[[-458.58306944]\n",
      " [ -54.58789055]\n",
      " [ -54.37429095]]\n",
      "\n",
      "[[-464.66980923]\n",
      " [ -55.77533679]\n",
      " [ -55.56056578]]\n",
      "\n",
      "[[-470.81741641]\n",
      " [ -56.98588027]\n",
      " [ -56.76993722]]\n",
      "\n",
      "[[-477.02649967]\n",
      " [ -58.21997046]\n",
      " [ -58.00285474]]\n",
      "\n",
      "[[-483.29767375]\n",
      " [ -59.47806558]\n",
      " [ -59.25977655]]\n",
      "\n",
      "[[-489.63155958]\n",
      " [ -60.76063279]\n",
      " [ -60.54116982]]\n",
      "\n",
      "[[-496.02878427]\n",
      " [ -62.0681483 ]\n",
      " [ -61.84751076]]\n",
      "\n",
      "[[-502.4899812 ]\n",
      " [ -63.40109763]\n",
      " [ -63.17928488]]\n",
      "\n",
      "[[-509.01579011]\n",
      " [ -64.75997572]\n",
      " [ -64.53698713]]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-509.01579011],\n",
       "       [ -64.75997572],\n",
       "       [ -64.53698713]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(X, y, theta_null, iterations, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RCs6EmWhYPM-"
   },
   "source": [
    "## Stretch Goals\n",
    "\n",
    "If you happen upon the most useful resources for accomplishing this challenge first, I want you to spend time today studying other variations of Gradient Descent-Based Optimizers.\n",
    "\n",
    "- Try and write a function that can perform gradient descent for arbitarily large (in dimensionality) multiple regression models. \n",
    "- Create a notebook for yourself exploring these topics\n",
    "- How do they differ from the \"vanilla\" gradient descent we explored today\n",
    "- How do these different gradient descent-based optimizers seek to overcome the challenge of finding the global minimum among various local minima?\n",
    "- Write a blog post that reteaches what you have learned about these other gradient descent-based optimizers.\n",
    "\n",
    "[Overview of GD-based optimizers](http://ruder.io/optimizing-gradient-descent/)\n",
    "\n",
    "[Siraj Raval - Evolution of Gradient Descent-Based Optimizers](https://youtu.be/nhqo0u1a6fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Gradient Descent Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
